"""
DAG Airflow - Pipeline E-commerce Events
Orchestration complÃ¨te : VÃ©rifications â†’ Spark Analytics â†’ ClickHouse
FrÃ©quence : Toutes les heures

NOTE: Producer et Consumer tournent en continu dans Docker.
Ce DAG fait uniquement le traitement batch horaire.
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
import logging
import psycopg2
from clickhouse_driver import Client
from confluent_kafka.admin import AdminClient

# --------------------------
# CONFIGURATION DU DAG
# --------------------------
default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'start_date': datetime(2025, 10, 5),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'pipeline_orchestration_v2',
    default_args=default_args,
    description='Pipeline batch: VÃ©rifications â†’ Spark â†’ ClickHouse',
    schedule_interval='@hourly',
    catchup=False,
    tags=['ecommerce', 'batch', 'spark', 'clickhouse'],
)

# ==============================================
# TASK 1 : VÃ©rification de Kafka
# ==============================================
def check_kafka_health():
    try:
        admin_client = AdminClient({'bootstrap.servers': 'kafka-simple:9092'})
        metadata = admin_client.list_topics(timeout=10)
        topics = list(metadata.topics.keys())
        
        logging.info("=" * 60)
        logging.info("ðŸ” HEALTH CHECK - KAFKA")
        logging.info("=" * 60)
        logging.info(f"âœ… Kafka opÃ©rationnel")
        logging.info(f"ðŸ“‹ Topics disponibles: {topics}")
        logging.info("=" * 60)
        
    except Exception as e:
        logging.error(f"âŒ Kafka inaccessible: {e}")
        raise

check_kafka = PythonOperator(
    task_id='check_kafka_health',
    python_callable=check_kafka_health,
    dag=dag,
)

# ==============================================
# TASK 2 : VÃ©rification de PostgreSQL
# ==============================================
def check_postgres_health():
    try:
        conn = psycopg2.connect(
            host='postgres-events',
            database='kafka_db',
            user='postgres',
            password='testpassword123'
        )
        
        with conn.cursor() as cur:
            cur.execute("SELECT 1")
            cur.execute("""
                SELECT COUNT(*) 
                FROM events 
                WHERE created_at >= NOW() - INTERVAL '1 hour'
            """)
            recent_count = cur.fetchone()[0]
            
            cur.execute("SELECT COUNT(*) FROM events")
            total_count = cur.fetchone()[0]
        
        conn.close()
        
        logging.info("=" * 60)
        logging.info("ðŸ” HEALTH CHECK - POSTGRESQL")
        logging.info("=" * 60)
        logging.info(f"âœ… PostgreSQL opÃ©rationnel")
        logging.info(f"ðŸ“Š Ã‰vÃ©nements (1h): {recent_count:,}")
        logging.info(f"ðŸ“Š Ã‰vÃ©nements (total): {total_count:,}")
        
        if recent_count == 0:
            logging.warning("âš ï¸  Aucun Ã©vÃ©nement dans la derniÃ¨re heure!")
        
        logging.info("=" * 60)
        
    except Exception as e:
        logging.error(f"âŒ PostgreSQL inaccessible: {e}")
        raise

check_postgres = PythonOperator(
    task_id='check_postgres_health',
    python_callable=check_postgres_health,
    dag=dag,
)

# ==============================================
# TASK 3 : VÃ©rification de ClickHouse
# ==============================================
def check_clickhouse_health():
    try:
        client = Client(
            host='clickhouse',
            port=9000,
            user='default',
            password='monMotDePasse123'
        )
        version = client.execute('SELECT version()')[0][0]
        
        logging.info("=" * 60)
        logging.info("ðŸ” HEALTH CHECK - CLICKHOUSE")
        logging.info("=" * 60)
        logging.info(f"âœ… ClickHouse opÃ©rationnel")
        logging.info(f"ðŸ“¦ Version: {version}")
        logging.info("=" * 60)
        
    except Exception as e:
        logging.error(f"âŒ ClickHouse inaccessible: {e}")
        raise

check_clickhouse = PythonOperator(
    task_id='check_clickhouse_health',
    python_callable=check_clickhouse_health,
    dag=dag,
)

# ==============================================
# TASK 4 : Calcul des mÃ©triques horaires
# ==============================================
def compute_hourly_metrics():
    logging.info("ðŸ”¹ DÃ©but du calcul des mÃ©triques horaires")
    try:
        conn = psycopg2.connect(
            host='postgres-events',
            database='kafka_db',
            user='postgres',
            password='testpassword123'
        )
        
        with conn.cursor() as cur:
            cur.execute("""
                SELECT 
                    event_type,
                    COUNT(*) as total_events,
                    AVG(request_latency_ms) as avg_latency,
                    COUNT(CASE WHEN status = 'ERROR' THEN 1 END) as error_count,
                    COUNT(CASE WHEN status = 'ERROR' THEN 1 END) * 100.0 / COUNT(*) as error_rate
                FROM events
                WHERE event_timestamp >= NOW() - INTERVAL '1 hour'
                GROUP BY event_type
                ORDER BY total_events DESC
            """)
            metrics = cur.fetchall()
        
        conn.close()
        
        logging.info("=" * 80)
        logging.info("ðŸ“Š MÃ‰TRIQUES HORAIRES")
        logging.info("=" * 80)
        
        for event_type, total, avg_lat, errors, error_rate in metrics:
            logging.info(f"""
ðŸ“ˆ {event_type}:
   â””â”€ Total: {total:,} Ã©vÃ©nements
   â””â”€ Latence moy: {avg_lat:.0f}ms
   â””â”€ Erreurs: {errors} ({error_rate:.2f}%)
            """.strip())
        
        logging.info("=" * 80)
        
    except Exception as e:
        logging.error(f"âŒ Erreur calcul mÃ©triques: {e}")
        raise

compute_metrics_task = PythonOperator(
    task_id='compute_hourly_metrics',
    python_callable=compute_hourly_metrics,
    dag=dag,
)

# ==============================================
# TASK 5 : Spark Batch Job
# ==============================================
run_spark_job = BashOperator(
    task_id='run_spark_analytics',
    bash_command="""
    echo "ðŸ”¥ Lancement du job Spark..."
    docker exec spark /opt/spark/bin/spark-submit \
      --master spark://spark:7077 \
      --deploy-mode client \
      --executor-memory 2g \
      --driver-memory 1g \
      --jars /opt/spark/work-dir/jars/postgresql-42.6.0.jar,/opt/spark/work-dir/jars/clickhouse-jdbc-0.3.2-patch1-all.jar \
      /opt/spark/work-dir/scripts/spark_processing.py
    echo "âœ… Job Spark terminÃ©"
    """,
    dag=dag,
)

# ==============================================
# TASK 6 : VÃ©rification des mÃ©triques ClickHouse
# ==============================================
def verify_clickhouse_metrics():
    try:
        client = Client(
            host='clickhouse',
            port=9000,
            user='default',
            password='monMotDePasse123'
        )
        tables = [t[0] for t in client.execute("SHOW TABLES")]
        
        logging.info("=" * 60)
        logging.info("ðŸ” VÃ‰RIFICATION CLICKHOUSE")
        logging.info("=" * 60)
        logging.info(f"ðŸ“‹ Tables disponibles: {tables}")
        
        if 'event_metrics' in tables:
            count = client.execute('SELECT COUNT(*) FROM event_metrics')[0][0]
            logging.info(f"ðŸ“Š MÃ©triques stockÃ©es: {count:,} lignes")
        else:
            logging.warning("âš ï¸  Table 'event_metrics' non trouvÃ©e")
        
        logging.info("=" * 60)
        
    except Exception as e:
        logging.error(f"âŒ Erreur ClickHouse: {e}")
        raise

verify_clickhouse = PythonOperator(
    task_id='verify_clickhouse_metrics',
    python_callable=verify_clickhouse_metrics,
    dag=dag,
)

# ==============================================
# TASK 7 : Notification de succÃ¨s
# ==============================================
def send_success_notification(**context):
    execution_date = context['execution_date']
    
    logging.info("=" * 80)
    logging.info("âœ… PIPELINE COMPLÃ‰TÃ‰ AVEC SUCCÃˆS")
    logging.info("=" * 80)
    logging.info(f"ðŸ“… ExÃ©cution: {execution_date}")
    logging.info(f"ðŸ”„ Flux: Health Checks â†’ MÃ©triques â†’ Spark â†’ ClickHouse")
    logging.info(f"â±ï¸  Prochain run: {context['next_execution_date']}")
    logging.info("=" * 80)

success_notification = PythonOperator(
    task_id='pipeline_success',
    python_callable=send_success_notification,
    provide_context=True,
    dag=dag,
)

# ==============================================
# FLUX DU PIPELINE
# ==============================================
[check_kafka, check_postgres, check_clickhouse] >> compute_metrics_task
compute_metrics_task >> run_spark_job >> verify_clickhouse >> success_notification
